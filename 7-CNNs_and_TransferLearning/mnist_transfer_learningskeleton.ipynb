{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "\n",
    "# Code to visualize the execution graph in the jupyter notebook\n",
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = b\"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_image(image,title=None):\n",
    "    plt.figure()\n",
    "    plt.imshow(image, cmap=\"gray\", interpolation=\"nearest\")\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "def plot_color_image(image):\n",
    "    plt.imshow(image.astype(np.uint8),interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "def getActivations(layer,stimuli):\n",
    "    units = sess.run(layer,feed_dict={X:np.reshape(stimuli,[1,784],order='F')})\n",
    "    plotNNFilter(units)\n",
    "\n",
    "# visualize convolution activations\n",
    "def plotNNFilter(units):\n",
    "    filters = units.shape[3]\n",
    "    #print (filters)\n",
    "    plt.figure(1, figsize=(20,20))\n",
    "    n_columns = 6\n",
    "    n_rows = math.ceil(filters / n_columns) + 1\n",
    "    for i in range(filters):\n",
    "        plt.subplot(n_rows, n_columns, i+1)\n",
    "        plt.title('Filter ' + str(i))\n",
    "        plt.imshow(units[0,:,:,i], interpolation=\"nearest\", cmap=\"gray\")\n",
    "    \n",
    "\n",
    "# visualize convolution kernels\n",
    "def plotConvLayerWeights(conv_layer_name, sess):\n",
    "    kernel = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, conv_layer_name)[0]\n",
    "    #print (kernel)\n",
    "    weights = sess.run(kernel)\n",
    "    #print (weights)\n",
    "    print(kernel.shape)\n",
    "    \n",
    "    n_filters = int(kernel.shape[3])\n",
    "    n_base_layers = int(kernel.shape[2])\n",
    "    plt.figure(figsize=(20,20))\n",
    "    for j in range(n_base_layers):\n",
    "        for i in range(n_filters):\n",
    "            plt.subplot(n_base_layers, n_filters, j * n_filters + i + 1)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            plt.imshow(weights[:,:,j,i].reshape(kernel.shape[0],kernel.shape[1]), cmap='gray', interpolation='nearest')\n",
    "            plt.title(str(i+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select 1459 and 023678 from MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-481f75177508>:8: Dataset.from_tensor_slices (from tensorflow.contrib.data.python.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.from_tensor_slices()`.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a17177978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a17177c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def prepare_data():\n",
    "    global  dataset_1459_train, \\\n",
    "            dataset_023678_train, \\\n",
    "            X_1459_train, \\\n",
    "            y_1459_train, \\\n",
    "            X_1459_test, \\\n",
    "            y_1459_test, \\\n",
    "            X_023678_test, \\\n",
    "            y_023678_test, \\\n",
    "            X_023678_train, \\\n",
    "            y_023678_train\n",
    "    mask_1459_train = np.logical_or.reduce([mnist.train.labels == v for v in [1,4,5,9]])\n",
    "    mask_1459_test  = np.logical_or.reduce([mnist.test.labels  == v for v in [1,4,5,9]])\n",
    "    X_1459_train = np.compress(np.array(mask_1459_train), mnist.train.images, axis=0)\n",
    "    #print(X_1459_train)\n",
    "    _, y_1459_train = np.unique(mnist.train.labels[mask_1459_train], return_inverse=True) # 1,4,5,9 to 0,1,2,3\n",
    "    dataset_1459_train = tf.contrib.data.Dataset.from_tensor_slices((X_1459_train,y_1459_train))\n",
    "    \n",
    "    X_1459_test = np.compress(np.array(mask_1459_test), mnist.test.images, axis=0)\n",
    "    _, y_1459_test = np.unique(mnist.test.labels[mask_1459_test], return_inverse=True)\n",
    "    \n",
    "    mask_023678_train = np.logical_or.reduce([mnist.train.labels == v for v in [0,2,3,6,7,8]])\n",
    "    mask_023678_test  = np.logical_or.reduce([mnist.test.labels  == v for v in [0,2,3,6,7,8]])\n",
    "    X_023678_train = np.compress(np.array(mask_023678_train), mnist.train.images, axis=0)\n",
    "    _, y_023678_train = np.unique(mnist.train.labels[mask_023678_train], return_inverse=True) # 1,4,5,9 to 0,1,2,3\n",
    "    dataset_023678_train = tf.contrib.data.Dataset.from_tensor_slices((X_023678_train,y_023678_train))\n",
    "    #print(dataset_023678_train)\n",
    "\n",
    "    X_023678_test = np.compress(np.array(mask_023678_test), mnist.test.images, axis=0)\n",
    "    _, y_023678_test = np.unique(mnist.test.labels[mask_023678_test], return_inverse=True)\n",
    "    #print(y_023678_train[0])\n",
    "\n",
    "prepare_data()\n",
    "\n",
    "# for example\n",
    "plot_image(X_1459_test[2].reshape(28,28),title=y_1459_test[2])\n",
    "plot_image(X_023678_test[2].reshape(28,28),title=y_023678_test[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "height = 28\n",
    "width = 28\n",
    "channels = 1\n",
    "n_inputs = height * width\n",
    "n_outputs = 4\n",
    "\n",
    "reset_graph()\n",
    "prepare_data()\n",
    "\n",
    "# The random seed that defines initialization.\n",
    "#SEED = 42\n",
    "\n",
    "conv1_weights = tf.Variable(\n",
    "  tf.truncated_normal([5, 5, channels, 32],  # 5x5 filter, depth 32.\n",
    "                      stddev=0.1,\n",
    "                      seed=42))\n",
    "\n",
    "conv1_biases = tf.Variable(tf.zeros([32])) # Bias variable\n",
    "\n",
    "conv2_weights = tf.Variable(\n",
    "  tf.truncated_normal([5, 5, 32, 64],\n",
    "                      stddev=0.1,\n",
    "                      seed=42))\n",
    "conv2_biases = tf.Variable(tf.constant(0.1, shape=[64])) # Bias vairable for the next layer\n",
    "\n",
    "#inputs layer\n",
    "with tf.name_scope(\"inputs\"):\n",
    "    X = tf.placeholder(tf.float32, shape=[None, n_inputs], name=\"X\")\n",
    "    X_reshaped = tf.reshape(X, shape=[-1, height, width, channels]) # Reshaped image\n",
    "    y = tf.placeholder(tf.int32, shape=[None], name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"conv\"):\n",
    "    conv1 = tf.layers.conv2d(X_reshaped, kernel_size=[5, 5],filters=32, strides=1,activation=tf.nn.relu,padding='SAME')\n",
    "    # Bias and rectified linear non-linearity.\n",
    "    #relu = tf.nn.relu(tf.nn.bias_add(conv1, conv1_biases))\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "    conv2 = tf.layers.conv2d(pool1, kernel_size=[5, 5],filters=64, strides=1,activation=tf.nn.relu,padding='SAME')\n",
    "    #relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_biases))\n",
    "    \n",
    "\n",
    "with tf.name_scope(\"pool3\"):\n",
    "    pool3 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "    pool3_flat = tf.reshape(pool3, [-1, 7 * 7 * 64])\n",
    "\n",
    "with tf.name_scope(\"fc1\"):\n",
    "    fc1 = tf.layers.dense(pool3_flat, 16, name=\"fc1\", activation=tf.nn.relu)\n",
    "\n",
    "with tf.name_scope(\"fc2\"):\n",
    "    fc2 = tf.layers.dense(fc1, 10, name = \"fc2\", activation = tf.nn.relu)\n",
    "\n",
    "with tf.name_scope(\"output\"):\n",
    "    logits = tf.layers.dense(fc2,4, name = \"logits\", activation = tf.nn.relu)\n",
    "    Y_proba = tf.nn.softmax(logits, name = \"Y_proba\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "with tf.name_scope(\"init_and_save\"):\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train 1459"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "0 Train accuracy: 0.996306 Test accuracy: 0.997511\n",
      "1 Train accuracy: 0.995759 Test accuracy: 0.996267\n",
      "2 Train accuracy: 0.996716 Test accuracy: 0.997511\n",
      "3 Train accuracy: 0.997309 Test accuracy: 0.996765\n",
      "4 Train accuracy: 0.998449 Test accuracy: 0.998009\n",
      "Finished training\n",
      "Saving...\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x105802128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 10\n",
    "\n",
    "print(\"Training...\")\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    batched_dataset = dataset_1459_train.batch(batch_size)\n",
    "\n",
    "    iterator = batched_dataset.make_initializable_iterator()\n",
    "    next_batch = iterator.get_next()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        sess.run(iterator.initializer)\n",
    "        while True:\n",
    "            try:\n",
    "                X_batch, y_batch = sess.run(next_batch)\n",
    "                sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                # finished running through dataset\n",
    "                break\n",
    "\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_1459_train, y: y_1459_train})\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_1459_test, y: y_1459_test})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "        \n",
    "    print(\"Finished training\")\n",
    "\n",
    "    print(\"Saving...\")\n",
    "    save_path = saver.save(sess, \"./my_model_1459.ckpt\")\n",
    "        \n",
    "    # for example\n",
    "    inferred = sess.run(Y_proba, feed_dict={X: [X_1459_test[0]]})\n",
    "    plot_image(X_1459_test[0].reshape(28,28), \"Predicted %d, truth %d\"%(np.argmax(inferred), y_1459_test[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the graph for 023678"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "prepare_data()\n",
    "\n",
    "# restore the graph of 1459\n",
    "restore_saver = tf.train.import_meta_graph(\"./my_model_1459.ckpt.meta\")\n",
    "\n",
    "# reuse the inputs (X,y)\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"inputs/X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"inputs/y:0\")\n",
    "# reuse the FC1 layer\n",
    "fc1 = tf.get_default_graph().get_tensor_by_name(\"fc1/fc1/Relu:0\")\n",
    "# reuse the FC2 layer\n",
    "fc2_reuse = tf.get_default_graph().get_tensor_by_name(\"fc2/fc2/Relu:0\")\n",
    "\n",
    "# continue the 023678 graph from FC2...\n",
    "with tf.name_scope(\"fc2_023678\"):\n",
    "    fc2 = tf.layers.dense(fc1,10,name=\"fc2_023678\", activation = tf.nn.relu)\n",
    "\n",
    "with tf.name_scope(\"softmax_023678\"):\n",
    "    logits = tf.layers.dense(fc2, 6, name=\"output_023678\",activation = tf.nn.relu)\n",
    "    Y_proba = tf.nn.softmax(logits, name=\"Y_proba_023678\")\n",
    "\n",
    "with tf.name_scope(\"train_023678\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    optimizer = tf.train.AdamOptimizer(name=\"adam_023678\")\n",
    "## try to freeze everything but fc2, softmax\n",
    "    #train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"fc2_023678|softmax_023678\")\n",
    "    #training_op = optimizer.minimize(loss, var_list=train_vars)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval_023678\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "with tf.name_scope(\"init_023678\"):\n",
    "    init = tf.global_variables_initializer()\n",
    "    new_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train transferred 023678"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 023678...\n",
      "0 Train accuracy: 0.992199 Test accuracy: 0.988633\n",
      "1 Train accuracy: 0.996372 Test accuracy: 0.992645\n",
      "2 Train accuracy: 0.997218 Test accuracy: 0.99348\n",
      "3 Train accuracy: 0.996946 Test accuracy: 0.993146\n",
      "4 Train accuracy: 0.994769 Test accuracy: 0.990639\n",
      "Finished training\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a53f61dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 10\n",
    "\n",
    "print(\"Training 023678...\")\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    batched_dataset = dataset_023678_train.batch(batch_size)\n",
    "\n",
    "    iterator = batched_dataset.make_initializable_iterator()\n",
    "    next_batch = iterator.get_next()\n",
    "    \n",
    "#     saver.restore(sess, \"./my_model_1459.ckpt\")\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        sess.run(iterator.initializer)\n",
    "        while True:\n",
    "            try:\n",
    "                X_batch, y_batch = sess.run(next_batch)\n",
    "                #print(y_batch)\n",
    "                sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                # finished running through dataset\n",
    "                break\n",
    "\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_023678_train, y: y_023678_train})\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_023678_test, y: y_023678_test})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "        \n",
    "    print(\"Finished training\")\n",
    "\n",
    "    # for example\n",
    "    inferred = sess.run(Y_proba, feed_dict={X: [X_023678_test[0]]})\n",
    "    plot_image(X_023678_test[0].reshape(28,28), \"Predicted %d, truth %d\"%(np.argmax(inferred), y_023678_test[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Feature Maps\n",
    "The following code is useful to look into the activations of convolutional and pooling layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    saver.restore(sess, \"./my_model_1459.ckpt\")\n",
    "    getActivations(tf.get_default_graph().get_tensor_by_name(\"correct:0\"), X_1459_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the convolution kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_1459.ckpt\")\n",
    "    \n",
    "    #plotConvLayerWeights(,sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the Execution Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "# restore the graph of 1459\n",
    "restore_saver = tf.train.import_meta_graph(\"./my_model_1459.ckpt.meta\")\n",
    "\n",
    "show_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "testenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
